IBM Machine Learning Course Notes
=================================

NOTE: I need to fix that issue that comes up everytime I load an `*.R` or an `*.Rmd` script, and presumably any python scripts too.

Module 1
--------

### Intro to ML ###

Use ML to:

- learn, from medical data, whether a tumor cell is benign or cancerous
- learn, how banks use financial data to decide whether a loan ought to be approved or not

We will hold your hand throughout this entire process. If you want to get a job in ML, you will absolutely need substantially more 
experience than we can offer you in this course!

So how is this simple example different from polygenic risk scoring and calculation?
  - is it that different?

So using algorithms, we can train the computer to recognize relevant factors, to be able to distinguish
important differences in multiple data points.

So... with PRS... we use the results of GWAS, to calculate the genetic effect for each SNP, and can then build
a model which will predict an individuals genetic predisposition to developing ADHD.

Conceptually it seems like the same basic idea.

- input data (genotypes, and phenotypes)
  - use a regression equation to calculate the risk/effect associated with individual variant
  - apply that risk to an independent sample
  - predict the risk or likelihood of an individual developing that disorder - or predict their genetic
    predisposition
  - PRS goes one step further I guess, and can use the calculated risk for a whole sample to estiamte the
    varaince explained in a target variable - either the same phenotype, or another, to examine whether the
    PRS is statistically associated and to what extent PRS can predict or explain another phenotype

We can use machine learning, to

So...

The key is that regression is only one of (the comparitively simpler) methods of building a model to predict
some outcome.

Could I predict an individuals likelihood of being diagnosed with ADHD, using environmental factors?
Could I predict an individuals likelihood of being diagnosed with ADHD, using genetic factors?

Could I compare these?

### Python for ML ###

Packages/libraries for ML in python:

- Pandas
- Numpy
- Scipy
- Scikit learn
- MatPlotlib

Using libraries, esp Scikit learn, can dramatically reduce the challenge and coding required to perform
machine learning (almost a shame, really).

Supervised:

- a named and labelled dataset - we can perform regressions, etc. to calculate betas which
  predict outcomes 

Unsupervised:

- unlabelled data - more sophisticated algorithms
- mining for observations and insights that are invisible to the human eye (eigenvectors? Decomposition?)

most common unsupervised methods:

- dimension reduction (eignevectors)
- density estimation
- market basket analysis
- clustering
  - discovering structure
  - summarisation
  - anomaly detection

Module 2
--------

### Regression ###

I will be interesting to see how much of this I understand, and if/where there are any gaps in my knowledge...

So, it sounds like we'll be estimating an effect on the dependent variable.

It's machine learning - it's all just terminology.

Yep, regression builds a model which can use X, to predict Y. That's all there is to it.

Just using data science/industry language to describe the concepts I already know. Love it.

### Linear Regression ###

Apparently don't need 'linear algebra' - interesting.

Nice introduction, but a few helpful explanations.

    y = intercept + slope(x)

The intercept and slope are known as the 'parameters' and/or the 'coefficients' of the model. Very helpful and clear information.

To find the line of best fit we have find the `MSE`, the Mean Squared Error (is this basically the same as ordinary least squares?)

So, after we've estimated our parameters, we can now estimate the value of a new depedent variable, based on its value of X.

NOTE: this will, as in the explanation of error terms, return a value with an error margin based on the error in the dataset.

In machine learning, no doubt this is another reason why huge datasets are useful. In a lot of scientific research you're interested in whether an effect exists and/or is statistically significant. In data science you're interested in producing predictions as accurately as possible - so the more correlates and error you can account for, and the more data you have, the better and more accurate your prediciton will be
- which is exactly the same as PRS!

So it's worth noting that I already understand the 'basics' of machine learning. I understand the concept of prediction, using data, and thinking about it, its becoming clear why massive data is so crucial to machine learning, compared to traditional scientific research and in particular clinical research and statistics.

Also worth noting that I'm really only familiar with the *most basic* machine learning methods - if you count multivariate regression and structural equation models as basic.

### Model Evaluation ###

So this is about estimation of error and accuracy of our data and models.

So, you can use linear models to get results, which can be measured for accuracy in the same way you measure the accuracy of your decision trees. So you can... *brain hurting*...

You can use layers of regression prediction estimates in decision trees? And you can use similar principles as in decision trees to optimise the order and sequence of predictions from regressions, to create an algorithm that predicts outcomes, based on a given set of inputs? - Ah - 'accuracy' - that's what I'm talking about here. The accuracy of the model, more or less.

However, the outputs of regression models could still be used to make decisions in a decision tree like structure right?

### train and test set ###

training accuracy: the accuracy of the model is predicting values from a dataset that it has been trained on
  - interpret with caution, over-fitting the model can cause it to be to specific to the training set, and not generalizable

out-of-sample accuracy: the accuracy of the model in predicting values from a dataset that it has *NOT* been trained on

### Train test split ###

more accurate out-of-sample accuracy


Ah I see. In the first case, the test set *is* a portion of the set that was used for training - i.e. it is a part of the training sample.

In test/train split - the test set is *absent* from the training data set.

Oh man, who would test their model on data they used to build the model? When could that possibly be a good idea?

### K fold cross validation ###

Even better?

Ah I see... so is it a kind of bootstrapping/resampling kind of technique?

It must also increase power, because it allows you to make use of *all* the data, independently, in both the training and the test data set.


### Accuracy metrics ###

R^2 ... so the correlation - or well, the covariance.

### Multiple Linear Regression ###

Theta = parameters = weight vector

We can represent multiple variables in a vector set.

The theta set, and the feature set.

X is the feature set, and is the transpose of Theta.

OK cool, so Theta0 is the intercept, so X[1] == 1. This ensures it always returns the value of theta0.

So the regression line in multiple regression is called the 'hyperplane'...

Looking again to reduce the MSE âˆ‘(Y - muY)/n

OLS and/or Optimization (maximum likelihood?)

OLS is apparently really slow for N > 10k

Too many variables can lead to overfitting.

### Non-linear regression ###

There are infinitely many variations of polynomial regression.

No doubt you have to be careful to select the model that best fits the data in a way that isn't just cherry picking the data to fit your biases.

Polynomial can be transformed into a linear regression, and therefore, linear regression principles can be applied to the dataset.

Polynomial is a special case of multiple regression, and it is *not* the same as non-linear regression.

OK cool, non-linear functions are non-linear functions with respect to theta, *not x*.

It's really just an extention of the idea that when the data is more systematic, or has some clear and/or simple predictable pattern, it's easier to work with.

Same thing in programming I guess, when you have repeatability, it can be simplified in a loop or other function.

How to determine non-linearity?

- visually examining all independent variables against the dependent variable
- calculate the correlation

lol. There's a lot of stuff not in the scope of this course.
- I wonder if any of my stats text books have information on how to transform, or fit polynomials to the data

I should be taking note of what's not covered in the couse, so I can explore the topics further.

Module 3
--------

### Classification ###

Categorising unknown items into a discrete set of classes. I suppose they'll clarify further in the next video?

Just more modelling of independent variable onto a set of categorical, or binary, dependent variables.

Classification algorithms:

- Decision trees
- Naive Bayes
- Linear Discriminant analysis
- k-nearest neighbour
- Logistic regression
- Neural networks
- Support Vector Machines

---

### k-nearest neighbours ###

So, it's a multivariate regression, which attempts to classify a data point by taking the average of its nearest neighbours. Simple and interesting.

So it's based on the assumption that data points that are closer to each other are more similar - this is an assumption right?

Calculating the similarity of datapoints can be done using 'euclidean distance'.

Oh, ok, so you actually calculate the distance of 'k' (the unknown parameter) to *all* data points in the dataset (again, more samples, more accuracy).

Find those point nearest, predict the parameter value of the unknown data point using the most popular response from the nearest neighbours.

So no doubt this is quite sensitive to the number neighbours you decide to include as 'nearest', and probably requires some finese as to how to select the appropriate number of neighbours?

So, we need to:

1. Select the correct 'K'
2. Calculate the similarity between each neighbour

We use 'minkowski distance' to calculate the distance between data points, which gives us the euclidean distance.

OK, so not as familiar now, but still doable. Excited to learn... no really!

Dis(x1, x2) = âˆš âˆ‘ (x1 - x2)^2

So by squaring, and taking the square root, you always return a positive value.

It's basic linear algebra, 'euclidean distance' - ah I see now.

How do we choose the right 'k' (the number of nearest neighbours).

Not enough - reduces accuracy, highly error prone, overly complicated - prone to overfitting

Hi value - overly generalised

How to find the best value for 'k'?

By calculating the accuracy of the data/model

- reserve a subset of test data to calculate accuracy

k-nearest neighbours can predict values on continuous data also, by simply calculating the mean or median of a set of k values (mean or median, how to decide?)

---

### Evaluation metrics in classification ###

Eval metrics calculate classification accuracy

pass test set to model, find predicted models. But how accurate?

We compare value in test set, to the predicted values. 

Different model evaluation metrics here are three examples:

- Jaccard index
- F1-score
- Log Loss

#### Jaccard index  ####

- the size of the 'intersection' of predicted vs test values, divided by the union of the set of predicted and set of test values.

So for example, if you had 10 points in each set, with 8 of them overlapping. The accuracy would be determined by:

8/10+(10-8) = 0.66

#### F1 score ####

A confusion matrix, which is identical to a power/significant ratio set. 

- Top left is a true positive
- Bottom right is a true negative
- Top right is a false negative
- Bottom left is a false positive

Called it.

So, the precision is the proportion of true positives to the total number of predicted positives (TP+FP).

So, the recall is the proportion of true positives to the total number of true values (TP+FN).

So, we can calculate the Precision and Recall for both true and false classifiiers.

We can then calculate the F1 score for both, based on the precision and recall values.

    Precision: TP/(TP+FP)
    Recall:    TP/(TP+FN)
    F1:        2*(prc*rec)/(prc+rec)

Note: you can have a low precision, and a high recall (or vice versa), and still obtain a reasonably good F1 score, so I suspect this value must be interpreted with caution.

#### Log Loss ####

When the predicted value is a probability estimate of obtaining a given class label (discrete outcome).

In logistic regression, the output can be the probability of 'customer churn', or the probability of success vs failure, or whatever

Log loss is the performance of a classifiers ability to confidently predict the correct output.

    (y*log(y) + (1-y)*log(1-y))

We calculate log loss for each independent prediction, then take the average of the sum total of log loss estimates. The higher the 'log loss', the poorer the performance of the model.

The lower the log loss, the better the accuracy of the model.

---

### Decision Trees ###

A decision algorithm, which makes decisions for a given data point, based on its characteristics determined by other (significantly relevant) characteristics.

Building decision trees: Recursive partitioning to classify data.

#### example ####

- the sex attribute has a stronger relationship (correlation) with the outcome (drug selection)
  - In data science terms - it is more *pure*, *significant* or *predictive* than cholesterol

In other words, sex has a stronger effect on drug selection, than cholesterol.

However, after sex, cholesterol is then able to further split males into each drug category completely

thus, by partitioning by sex, and then cholesterol, we can confidently predict the appropriate drug type for males.

---

So, decision trees are all about:

> Using recursive parititioning to split the data in a way that minimized the 'impurity' at each step

What amazes me is that changing the order of the covariates in the tree changes the outcome.

Like - using sex as the first node is *better* than using cholesterol as the first node, even if you introduce sex afterwards.

Interesting.

#### Entropy ####

Entropy is the amount of randomness in a given node. The best decision trees minimize the entropy of each node.

Sounds like a computationally intensive process, iterating through combinations of trees to find the one that minimises the entropy of each node.

    Entropy = - p(A)log(p(A)) - p(B)log(p(B))

Where A and B, are the proportions or probabilities of each categorical outcome for a given node.

Information gain is the Entropy of the tree before the split (partition based on some covariate) minus the *weighted* entropy of the tree after the split.

So, basically if it's negative then it's a bad split, and if it's positive it's a good split

And the information gain that is highest between two different splits, is the better split for that dataset.

Ah yeah, OK, so 'weighted' means that each branch in the node, is weighted the proportion of data points which are on that branch (so, for example, the proportion of females multiplied by the entropy score for females plus the proportion of males multiplied by the entropy score for males).

No doubt there's a way to automate this process, and computationally calculate the best tree design, without having to do everything manually.

Oh man. I *really* need to do some projects on my own. Seriously.

---

### Logistic Regression ###


What is log regress?

What is it for?

Classifying records of a data set.

When do we use it?

Binary data

When you need the probability of your prediction.

When you need to understand the impact of a feature.

OK, so can I summarise situtations where I need a decision tree, linear, or logistic regression?
- I think I need to practise with them some more

### Linear vs Logistic Regression ###

The sigmoid function - key to logistic regression.

Wow this is intense.

So sigmoid uses the same symbol as standard deviation. But it's not standard deviation?

So linear regression is used to predict a continuous variable.

Decision trees attempt to predict discreet outcomes by discovering the tree structure which maximises the information gain.

Logistic regression attempts to calculate the probability of an outcome, based on the input variables.

Still not 100% clear on when you want to use decision trees, and when you want to use logistic regression.

Use decision trees, when you want to assign categorical variables to a class based on input.

Use logistic regression when you want to predict the probability of a given *outcome* occurring, given a set of inputs.

Maybe some real world examples will help to clarify.

So we're up 3 out 20+ Machine learning methods... heavy.

    https://blog.bigml.com/2016/09/28/logistic-regression-versus-decision-trees/

Right, should've been obvious. so applying either to a categorical outcome will successfully produce a model.

The question is when is one more *appropriate* than the other...

> They differ in the way they produce *decision boundaries*.

OK, one obvious difference... Is the bisection of the data points a relatively *linear boundary*? If not logistic regression will under perform.

If the boundary is clearly linear, then decision trees risk overfitting the data

### Training Logistic Regression ###

Change the parameters - to optimise the accuracy of the model

Formulate the cost function

Yep, I really need a notepad to write down these equations.

cost function = the Mean Squared Error

    cost(Ë†y, y) = 1/2 (sigmoid(Ã˜ X) - y)^2

    j(Ã˜) = 1/m âˆ‘ cost(Ë†y, y)

How do we find the best parameters for our model?
- by minimizing the cost function

How do minimize the cost function?
- By using a minimization approach
  - Gradient descent, for example

Definitely look at this in more detail.

> In order to estimate the class of a data point, we need some sort of guidance on what would be the most probable class for that data point. For this, we use Logistic Regression.

> Logistic Regression is a variation of Linear Regression, useful when the observed dependent variable, y, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.

Logistic regression fits an S shaped curve (interesting). I actually haven't really spent much time on logistic regression. I need to change this.

Logistic regression calculates the probability of a 'class' (a discrete outcome), given some independent variables.

P(Y = 1| X) = sigmoid(transpose(Ã˜) X)

e^(Ã˜X)/1 + e^(Ã˜X)

So, as e^(Ã˜X) increases, sigmiod approaches 1. So the larger the sigmoid value, the higher the probability. CoOl. *But - if I understand this correctly, can never actually be exactly 1, unless the result of the sigmoid function approaches infinity! (What actually would happen if sigmoid is infinity? What's inf/inf? Ah OK, so it gets weird. BUT - as sigmoid gets infinitely large (but is still some finite value) it will never be exactly one, but will eventually as close as practicable as to make no difference in the context of most statistical analysis*

Man, I feel like I've never learned about logistic regression before? Is it covered in Bulmer stats? I should check it out (in fact I should probably read that book again!)

Surprisingly more complicated than linear regression.

Note: In my Sleep study, I have noticed that the binary traits (like Insomnia), were generally more powered - I think snoring was also a binary trait.

[//]: # ( Is there something about the logistic regression which maybe more prone to bias? Or to say it another way... Is there cause to doubt my PRS results? I do wonder! )

Packages I'll be using for Machine Learning:

    import pandas as pd
    import pylab as pl
    import numpy as np
    import scipy.optimize as opt
    from sklearn import preprocessing
    %matplotlib inline
    import matplotlib.pyplot as plt

So we import our dataset.
Select variables.
Define X and Y.
Transform if necessary.
Create training and test datasets.

*Very useful information:*

> Let's build our model using LogisticRegression from the Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including â€˜newton-cgâ€™, â€˜lbfgsâ€™, â€˜liblinearâ€™, â€˜sagâ€™, â€˜sagaâ€™ solvers. You can find extensive information about the pros and cons of these optimizers if you search it in the internet.

> The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem of machine learning models. C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization. Now let's fit our model with train set:


    Precision: TP/(TP+FP)
    Recall:    TP/(TP+FN)
    F1:        2*(prc*rec)/(prc+rec)

### Support Vector Machines ###




---

### Other stuff to note ###

- I don't really want to use Jupyter labs and notebooks. I want to code in the terminal, or in vim. I'd rather have a job where I was working through terminal and command line interfaces, rather than notebooks and browsers

### Stuff to follow up on ###

Euclidean distance (k-nearest neighbour)

Logistic regression - training, minimizing the cost function, gradient descent

    https://developers.google.com/machine-learning/crash-course/ - another potentially good resource

### Classification algorithms ###

Subjects to explore in greater depth, over time.

- Decision trees
- Naive Bayes
- Linear Discriminant analysis
- k-nearest neighbour
- Logistic regression
- Neural networks
- Support Vector Machines











